{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ff8fa9-bf9b-441f-8ec0-b8e49ddb17d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.transforms as transforms\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1601ee72-44b2-4d93-be80-91f78fc93449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a seed for reproducible results\n",
    "np.random.seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727ac7aa-885f-47d4-abd3-8134fc28b5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available, then MPS, otherwise use CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    # cluster path\n",
    "    vector_path = \"../scratch/vector\"\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "    # local path\n",
    "#    vector_path = \"../data.nosync/vector\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    # local path\n",
    "    vector_path = \"../data.nosync/vector\"\n",
    "\n",
    "print(f\"Device set to: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aa9a4b4-07d6-4121-8db4-63dc413c2156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operators are always specified in this order\n",
    "operator_order = (\"elimination\", \"aggregation\", \"typification\", \"displacement\", \"enlargement\", \"simplification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02b637e4-16aa-431f-8e36-bfee5730d3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DIN font for plots if working locally\n",
    "if not torch.cuda.is_available():\n",
    "    plt.rcParams[\"font.family\"] = \"DIN Alternate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ffaec-0dc4-4015-9793-2da90d41184a",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db5da42-2c67-49bb-8cb6-13ad1f4a92cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up a Dataset object for DataLoader\n",
    "class BuildingVectorDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        '''Stores the directory and filenames of the individual .pt files.'''\n",
    "        super().__init__(path, transform)\n",
    "        # store directory of individual files\n",
    "        self.path = path\n",
    "        # get filenames of individual .pt files\n",
    "        self.filenames = [file for file in os.listdir(path) if file.endswith(\".pt\")]\n",
    "\n",
    "        # store transformation\n",
    "        self.transform = transform\n",
    "\n",
    "    def len(self):\n",
    "        '''Enables dataset length calculation.'''\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def get(self, index):\n",
    "        '''Enables indexing, returns HeteroData object which contains nodes, edges and labels.'''\n",
    "        # get filename associated with given index\n",
    "        filename = self.filenames[index]\n",
    "\n",
    "        # load the file with the filename\n",
    "        graph = torch.load(os.path.join(self.path, filename))\n",
    "\n",
    "        # TODO: slice graph.y with respect to the specified generalization operators in the __init__ method\n",
    "\n",
    "        # apply given transformation if specified\n",
    "        #if self.transform:\n",
    "        #    graph = self.transform(graph)\n",
    "\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4396e6b-bd1e-4b58-b0eb-1a6c29d6f965",
   "metadata": {},
   "source": [
    "### Model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3014c623-4ce2-41e3-9569-71809d8b70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class HeteroGNN(nn.Module):\n",
    "    def __init__(self, node_features, node_to_predict, metadata, n_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = pyg_nn.HGTConv(in_channels=node_features,\n",
    "                             out_channels=64, metadata=(metadata))\n",
    "        self.conv2 = pyg_nn.HGTConv(in_channels=64, out_channels=n_classes, metadata=(metadata))\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {k: torch.relu(x) for k, x in x_dict.items()}\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        return x_dict[node_to_predict]  # Only return the building predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e74bd3ff-a799-4d39-b366-0ae3e49b0f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 samples in the training set.\n",
      "10 samples in the training set.\n"
     ]
    }
   ],
   "source": [
    "# define path to training and validation data\n",
    "path_to_training_data = os.path.join(vector_path, \"training_data\")\n",
    "\n",
    "# composing various random transforms that should be applied to the data\n",
    "#transform = transforms.Compose([transforms.ToUndirected()])\n",
    "\n",
    "# construct training DataLoader\n",
    "training_set = BuildingVectorDataset(path_to_training_data, transform=None)\n",
    "training_loader = DataLoader(training_set, batch_size=1, shuffle=True)\n",
    "\n",
    "# construct validation DataLoader (no transformations, no shuffling)\n",
    "validation_set = BuildingVectorDataset(path_to_training_data, transform=None)\n",
    "validation_loader = DataLoader(training_set, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"{len(training_set):,} samples in the training set.\")\n",
    "print(f\"{len(training_set):,} samples in the training set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3298c9cd-6c0a-4cdb-8b95-b10f9d7006b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 building features, 2 road features, 6 operators\n"
     ]
    }
   ],
   "source": [
    "# extracting the relevant metadata from the data to set up the model\n",
    "node_to_predict = \"building\"\n",
    "n_building_features = training_set.get(0)[\"building\"][\"x\"].shape[1]\n",
    "n_road_features = training_set.get(0)[\"road\"][\"x\"].shape[1]\n",
    "node_features = {\"building\": n_building_features, \"road\": n_road_features}\n",
    "n_classes = training_set.get(0)[\"building\"][\"y\"].shape[1]\n",
    "\n",
    "print(f\"{n_building_features} building features, {n_road_features} road features, {n_classes} operators\")\n",
    "\n",
    "# construct the model\n",
    "model = HeteroGNN(node_features, node_to_predict=node_to_predict, metadata=training_set.get(0).metadata(), n_classes=n_classes)\n",
    "model.to(device)\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss() # Binary cross-entropy loss, applies a sigmoid internally and takes logits as input\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8af6e24b-6719-422b-bdeb-c9a4f0f89530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/10, step 1/10\n",
      "epoch 1/10, step 2/10\n",
      "epoch 1/10, step 3/10\n",
      "epoch 1/10, step 4/10\n",
      "epoch 1/10, step 5/10\n",
      "epoch 1/10, step 6/10\n",
      "epoch 1/10, step 7/10\n",
      "epoch 1/10, step 8/10\n",
      "epoch 1/10, step 9/10\n",
      "epoch 1/10, step 10/10\n",
      "epoch 1 finished, training loss: 501.149, validation loss: 6.862\n",
      "epoch 2/10, step 1/10\n",
      "epoch 2/10, step 2/10\n",
      "epoch 2/10, step 3/10\n",
      "epoch 2/10, step 4/10\n",
      "epoch 2/10, step 5/10\n",
      "epoch 2/10, step 6/10\n",
      "epoch 2/10, step 7/10\n",
      "epoch 2/10, step 8/10\n",
      "epoch 2/10, step 9/10\n",
      "epoch 2/10, step 10/10\n",
      "epoch 2 finished, training loss: 3.137, validation loss: 0.714\n",
      "epoch 3/10, step 1/10\n",
      "epoch 3/10, step 2/10\n",
      "epoch 3/10, step 3/10\n",
      "epoch 3/10, step 4/10\n",
      "epoch 3/10, step 5/10\n",
      "epoch 3/10, step 6/10\n",
      "epoch 3/10, step 7/10\n",
      "epoch 3/10, step 8/10\n",
      "epoch 3/10, step 9/10\n",
      "epoch 3/10, step 10/10\n",
      "epoch 3 finished, training loss: 0.712, validation loss: 0.711\n",
      "epoch 4/10, step 1/10\n",
      "epoch 4/10, step 2/10\n",
      "epoch 4/10, step 3/10\n",
      "epoch 4/10, step 4/10\n",
      "epoch 4/10, step 5/10\n",
      "epoch 4/10, step 6/10\n",
      "epoch 4/10, step 7/10\n",
      "epoch 4/10, step 8/10\n",
      "epoch 4/10, step 9/10\n",
      "epoch 4/10, step 10/10\n",
      "epoch 4 finished, training loss: 0.709, validation loss: 0.708\n",
      "epoch 5/10, step 1/10\n",
      "epoch 5/10, step 2/10\n",
      "epoch 5/10, step 3/10\n",
      "epoch 5/10, step 4/10\n",
      "epoch 5/10, step 5/10\n",
      "epoch 5/10, step 6/10\n",
      "epoch 5/10, step 7/10\n",
      "epoch 5/10, step 8/10\n",
      "epoch 5/10, step 9/10\n",
      "epoch 5/10, step 10/10\n",
      "epoch 5 finished, training loss: 0.706, validation loss: 0.705\n",
      "epoch 6/10, step 1/10\n",
      "epoch 6/10, step 2/10\n",
      "epoch 6/10, step 3/10\n",
      "epoch 6/10, step 4/10\n",
      "epoch 6/10, step 5/10\n",
      "epoch 6/10, step 6/10\n",
      "epoch 6/10, step 7/10\n",
      "epoch 6/10, step 8/10\n",
      "epoch 6/10, step 9/10\n",
      "epoch 6/10, step 10/10\n",
      "epoch 6 finished, training loss: 0.703, validation loss: 0.702\n",
      "epoch 7/10, step 1/10\n",
      "epoch 7/10, step 2/10\n",
      "epoch 7/10, step 3/10\n",
      "epoch 7/10, step 4/10\n",
      "epoch 7/10, step 5/10\n",
      "epoch 7/10, step 6/10\n",
      "epoch 7/10, step 7/10\n",
      "epoch 7/10, step 8/10\n",
      "epoch 7/10, step 9/10\n",
      "epoch 7/10, step 10/10\n",
      "epoch 7 finished, training loss: 0.700, validation loss: 0.698\n",
      "epoch 8/10, step 1/10\n",
      "epoch 8/10, step 2/10\n",
      "epoch 8/10, step 3/10\n",
      "epoch 8/10, step 4/10\n",
      "epoch 8/10, step 5/10\n",
      "epoch 8/10, step 6/10\n",
      "epoch 8/10, step 7/10\n",
      "epoch 8/10, step 8/10\n",
      "epoch 8/10, step 9/10\n",
      "epoch 8/10, step 10/10\n",
      "epoch 8 finished, training loss: 0.697, validation loss: 0.695\n",
      "epoch 9/10, step 1/10\n",
      "epoch 9/10, step 2/10\n",
      "epoch 9/10, step 3/10\n",
      "epoch 9/10, step 4/10\n",
      "epoch 9/10, step 5/10\n",
      "epoch 9/10, step 6/10\n",
      "epoch 9/10, step 7/10\n",
      "epoch 9/10, step 8/10\n",
      "epoch 9/10, step 9/10\n",
      "epoch 9/10, step 10/10\n",
      "epoch 9 finished, training loss: 0.693, validation loss: 0.691\n",
      "epoch 10/10, step 1/10\n",
      "epoch 10/10, step 2/10\n",
      "epoch 10/10, step 3/10\n",
      "epoch 10/10, step 4/10\n",
      "epoch 10/10, step 5/10\n",
      "epoch 10/10, step 6/10\n",
      "epoch 10/10, step 7/10\n",
      "epoch 10/10, step 8/10\n",
      "epoch 10/10, step 9/10\n",
      "epoch 10/10, step 10/10\n",
      "epoch 10 finished, training loss: 0.689, validation loss: 0.688\n",
      "Training time: 0.435 seconds\n"
     ]
    }
   ],
   "source": [
    "# number of epochs and batch size\n",
    "n_epochs = 10\n",
    "batch_size = 1\n",
    "\n",
    "total_samples = len(training_set)\n",
    "n_iterations = math.ceil(total_samples/batch_size)\n",
    "    \n",
    "# saving the losses from every epoch\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "    \n",
    "for epoch in range(n_epochs):\n",
    "    # tracking loss per epoch\n",
    "    training_running_loss = 0.0\n",
    "    n_training_batches = 0\n",
    "    validation_running_loss = 0.0\n",
    "    n_validation_batches = 0\n",
    "\n",
    "    # training phase\n",
    "    model.train()\n",
    "\n",
    "    for i, graph in enumerate(training_loader):\n",
    "        n_training_batches += 1\n",
    "        # target operators\n",
    "        operators = graph.y_dict[node_to_predict]\n",
    "        \n",
    "        # moving the features to device\n",
    "        graph = graph.to(device)\n",
    "        operators = operators.to(device)\n",
    "    \n",
    "        # empty the gradients\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        # forward pass\n",
    "        pred_operators_logits = model(graph.x_dict, graph.edge_index_dict) # compute predictions, calls forward method under the hood\n",
    "        loss = criterion(pred_operators_logits, operators) # calculate loss\n",
    "        training_running_loss += loss.item() # tracking running loss to keep track of the loss for every epoch\n",
    "    \n",
    "        # backward pass\n",
    "        loss.backward() # backpropagation\n",
    "        optimizer.step() # update the parameters\n",
    "    \n",
    "        # print information every few batches\n",
    "        if not (i + 1) % (n_iterations // 10):\n",
    "            print(f\"epoch {epoch+1}/{n_epochs}, step {i+1}/{n_iterations}\")\n",
    "\n",
    "    # validation phase\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for graph in validation_loader:\n",
    "            n_validation_batches += 1\n",
    "            # target operators\n",
    "            operators = graph.y_dict[node_to_predict]\n",
    "            \n",
    "            # moving the features to device\n",
    "            graph = graph.to(device)\n",
    "            operators = operators.to(device)\n",
    "\n",
    "            # prediction on the trained model results in logits\n",
    "            pred_operators_logits = model(graph.x_dict, graph.edge_index_dict) # compute predictions, calls forward method under the hood\n",
    "            # calculate and store validation loss\n",
    "            loss = criterion(pred_operators_logits, operators)\n",
    "            validation_running_loss += loss.item()\n",
    "    \n",
    "    # print information at the end of each epoch\n",
    "    training_loss_epoch = training_running_loss / n_training_batches\n",
    "    training_losses.append(training_loss_epoch)\n",
    "    validation_loss_epoch = validation_running_loss / n_validation_batches\n",
    "    validation_losses.append(validation_loss_epoch)\n",
    "    \n",
    "    print(f\"epoch {epoch+1} finished, training loss: {training_loss_epoch:.3f}, validation loss: {validation_loss_epoch:.3f}\")\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "print(f\"Training time: {end_time - start_time:,.3f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
