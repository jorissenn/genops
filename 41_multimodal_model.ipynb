{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee3a016-81f8-443d-b200-9616059a652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a3989-b0f0-4496-8567-e4435c5aa980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a seed for reproducible results\n",
    "np.random.seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8630a-e832-4fea-b0cf-fef86e34b62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available, then MPS, otherwise use CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    # cluster path\n",
    "    multimodal_path = \"../scratch/multimodal\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    # local path\n",
    "    multimodal_path = \"../data.nosync/multimodal\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    # local path\n",
    "    multimodal_path = \"../data.nosync/multimodal\"\n",
    "\n",
    "print(f\"Device set to: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9bc9a6-4bff-4444-af15-53e494907a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operators are always specified in this order\n",
    "operator_order = (\"elimination\", \"aggregation\", \"typification\", \"displacement\", \"enlargement\", \"simplification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ec790-06d8-49e5-a93f-3fad641d9b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DIN font for plots if working locally\n",
    "if not torch.cuda.is_available():\n",
    "    plt.rcParams[\"font.family\"] = \"DIN Alternate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d6d062-de0e-4c88-8518-27cf100f0866",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f0071-6616-45bc-ada7-d7f954be8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildingMultimodalDataset(Dataset):\n",
    "    def __init__(self, path, operators, transform=None):\n",
    "        '''Stores the directory and filenames of the individual raster (.npz) and vector (.pt) files.'''\n",
    "        # store directory of individual files\n",
    "        self.path = path\n",
    "        # store the path to the raster and vector files\n",
    "        self.raster_path = os.path.join(path, \"raster\")\n",
    "        self.vector_path = os.path.join(path, \"vector\")\n",
    "\n",
    "        # get filenames of the individual files\n",
    "        # potentially sort this to make sure that the samples line up?\n",
    "        self.raster_filenames = os.listdir(self.raster_path)\n",
    "        self.vector_filenames = os.listdir(self.vector_path)\n",
    "\n",
    "        # make sure that the samples line up\n",
    "        assert len(self.raster_filenames == self.vector_filenames)\n",
    "\n",
    "        # store indices of the operators within operator_order for slicing in the __getitem__ method\n",
    "        self.operators = sorted([operator_order.index(operator) for operator in operators if operator in operator_order])\n",
    "\n",
    "        # store transformation\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Enables dataset length calculation.'''\n",
    "        return len(self.raster_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''Enables indexing, returns graph and raster representation and generalization operator as label.'''\n",
    "        # load the raster sample associated with the given index\n",
    "        raster_filename = self.raster_filenames[index]\n",
    "        raster_sample_raw = np.load(os.path.join(self.raster_path, raster_filename))\n",
    "\n",
    "        # extract the rasters\n",
    "        focal_building_raster = raster_sample_raw[\"focal_building\"]\n",
    "        context_buildings_raster = raster_sample_raw[\"context_buildings\"]\n",
    "        roads_raster = raster_sample_raw[\"roads\"]\n",
    "\n",
    "        # stack the rasters to shape (3, n_pixels, n_pixels) and convert to tensor\n",
    "        raster_sample = np.stack([focal_building_raster, context_buildings_raster, roads_raster], axis=0)\n",
    "        raster_sample = torch.from_numpy(raster_sample).float()\n",
    "\n",
    "        # load the vector sample associated with the given index\n",
    "        vector_filename = self.vector_filenames[index]\n",
    "        vector_sample = torch.load(os.path.join(self.vector_path, vector_filename))\n",
    "\n",
    "        # extract the operators from the graph object\n",
    "        operators = vector_sample.y[self.operators]#.reshape(1, -1)\n",
    "\n",
    "        return raster_sample, vector_sample, operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c759b-a29e-4970-9962-54684478b60c",
   "metadata": {},
   "source": [
    "### Model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccde72b1-afe6-4b7a-97d1-701de9522a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture of best performing raster model\n",
    "class RasterModel(nn.Module):\n",
    "    pass\n",
    "\n",
    "# architecture of best performing vector model\n",
    "class VectorModel(nn.Module):\n",
    "    pass\n",
    "\n",
    "# load the trained raster model\n",
    "raster_model_path = \"\"\n",
    "raster_model = RasterModel()\n",
    "raster_model.load_state_dict(torch.load(raster_model_path))\n",
    "raster_model.eval()\n",
    "    \n",
    "# load the trained vector model\n",
    "vector_model_path = \"\"\n",
    "vector_model = VectorModel()\n",
    "vector_model.load_state_dict(torch.load(vector_model_path))\n",
    "vector_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0530f55-8b68-4bc2-baac-26b6b98b1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, raster_model, vector_model, n_raster_features, n_vector_features, n_classes):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.raster_model = raster_model\n",
    "        self.vector_model = vector_model\n",
    "        \n",
    "        # both models are already trained and only require gradient for fusion layers\n",
    "        for param in self.raster_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.vector_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # TODO: remove last linear layers (https://discuss.pytorch.org/t/custom-ensemble-approach/52024/4)\n",
    "        \n",
    "        # fusion layer\n",
    "        self.fusion_layer = nn.Linear(n_raster_features + n_vector_features, n_classes)\n",
    "\n",
    "    def forward(self, graph, raster):\n",
    "        raster_output = self.raster_model(block)\n",
    "        vector_output = self.vector_model(graph.x_dict, graph.edge_index_dict)\n",
    "\n",
    "        # concatenate along feature dimension\n",
    "        combined_features = torch.cat((raster_output, vector_output), dim=1)\n",
    "        result = self.fusion_layer(combined_features)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf4f96a-b9c1-486e-b07d-a83c885ac113",
   "metadata": {},
   "source": [
    "### Elimination model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ffbd5e-7ad4-4e11-969e-00cf2b35ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to training and validation data\n",
    "path_to_training_data = os.path.join(multimodal_path, \"training_data\", \"elimination\", \"training\")\n",
    "path_to_validation_data = os.path.join(multimodal_path, \"training_data\", \"elimination\", \"validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c8169-24e1-4261-ae05-69c411866c45",
   "metadata": {},
   "source": [
    "### Selection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e667bd-93a1-4456-88a3-78bf9e0110b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to training and validation data\n",
    "path_to_training_data = os.path.join(multimodal_path, \"training_data\", \"selection\", \"training\")\n",
    "path_to_validation_data = os.path.join(multimodal_path, \"training_data\", \"selection\", \"validation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
