{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ee3a016-81f8-443d-b200-9616059a652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from model_components.vit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "268a3989-b0f0-4496-8567-e4435c5aa980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a seed for reproducible results\n",
    "np.random.seed(69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f8630a-e832-4fea-b0cf-fef86e34b62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available, then MPS, otherwise use CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.empty_cache()\n",
    "    # cluster path\n",
    "    multimodal_path = \"../scratch/multimodal\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    # local path\n",
    "    multimodal_path = \"../data.nosync/multimodal\"\n",
    "\n",
    "print(f\"Device set to: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd9bc9a6-4bff-4444-af15-53e494907a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# operators are always specified in this order\n",
    "operator_order = (\"elimination\", \"aggregation\", \"typification\", \"displacement\", \"enlargement\", \"simplification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "737ec790-06d8-49e5-a93f-3fad641d9b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define DIN font for plots if working locally\n",
    "if not torch.cuda.is_available():\n",
    "    plt.rcParams[\"font.family\"] = \"DIN Alternate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d6d062-de0e-4c88-8518-27cf100f0866",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d80f0071-6616-45bc-ada7-d7f954be8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildingMultimodalDataset(Dataset):\n",
    "    def __init__(self, raster_path, vector_path, operators, transform=None):\n",
    "        '''Stores the directory and filenames of the individual raster (.npz) and vector (.pt) files.'''\n",
    "        # store the path to the raster and vector files\n",
    "        self.raster_path = raster_path\n",
    "        self.vector_path = vector_path\n",
    "\n",
    "        # get filenames of the individual files, sort the filenames to make them line up\n",
    "        self.raster_filenames = sorted(os.listdir(self.raster_path))\n",
    "        self.vector_filenames = sorted(os.listdir(self.vector_path))\n",
    "\n",
    "        # make sure that the samples line up\n",
    "        assert len(self.raster_filenames) == len(self.vector_filenames)\n",
    "\n",
    "        # store indices of the operators within operator_order for slicing in the __getitem__ method\n",
    "        self.operators = sorted([operator_order.index(operator) for operator in operators if operator in operator_order])\n",
    "\n",
    "        # store transformation\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Enables dataset length calculation.'''\n",
    "        return len(self.raster_filenames)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''Enables indexing, returns graph and raster representation and generalization operator as label.'''\n",
    "        # load the raster sample associated with the given index\n",
    "        raster_filename = self.raster_filenames[index]\n",
    "        raster_sample_raw = np.load(os.path.join(self.raster_path, raster_filename))\n",
    "\n",
    "        # extract the rasters\n",
    "        focal_building_raster = raster_sample_raw[\"focal_building\"]\n",
    "        context_buildings_raster = raster_sample_raw[\"context_buildings\"]\n",
    "        roads_raster = raster_sample_raw[\"roads\"]\n",
    "\n",
    "        # stack the rasters to shape (3, n_pixels, n_pixels) and convert to tensor\n",
    "        raster_sample = np.stack([focal_building_raster, context_buildings_raster, roads_raster], axis=0)\n",
    "        raster_sample = torch.from_numpy(raster_sample).float()\n",
    "\n",
    "        # load the vector sample associated with the given index\n",
    "        vector_filename = self.vector_filenames[index]\n",
    "        vector_sample = torch.load(os.path.join(self.vector_path, vector_filename))\n",
    "\n",
    "        # extract the operators from the graph object\n",
    "        operators = vector_sample.y[self.operators]\n",
    "\n",
    "        # reshape the operators associated with the graph\n",
    "        vector_sample.y = vector_sample.y[self.operators].reshape(1, -1)\n",
    "\n",
    "        return raster_sample, vector_sample, operators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5c759b-a29e-4970-9962-54684478b60c",
   "metadata": {},
   "source": [
    "### Model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccde72b1-afe6-4b7a-97d1-701de9522a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture of best performing raster model\n",
    "class MultiTaskViT(nn.Module):\n",
    "    def __init__(self, *, \n",
    "                 image_size=256, \n",
    "                 patch_size=32, \n",
    "                 num_classes, \n",
    "                 dim=512, \n",
    "                 depth=6, \n",
    "                 heads=16, \n",
    "                 mlp_dim=2048, \n",
    "                 pool='cls',\n",
    "                 channels, \n",
    "                 dim_head=64, \n",
    "                 dropout=0., \n",
    "                 emb_dropout=0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.classification_heads = nn.ModuleList([nn.Linear(dim, 1) for _ in range(num_classes)])\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        \n",
    "        # apply each classification head and concatenate the results along the final dimension\n",
    "        if isinstance(self.classification_heads, nn.ModuleList):\n",
    "            outputs = torch.cat([head(x).squeeze(-1).unsqueeze(1) for head in self.classification_heads], dim=1)\n",
    "            return outputs\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def get_n_parameters(self):\n",
    "        n_parameters = sum(p.numel() for p in self.parameters())\n",
    "        return n_parameters\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Multi-Task Vision transformer with {self.get_n_parameters():,} parameters\"\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, n_input_features, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            pyg_nn.Linear(n_input_features, n_input_features//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            pyg_nn.Linear(n_input_features//2, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# architecture of best performing vector model\n",
    "class MultiTaskHGT(nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_heads, num_layers, node_types, metadata, node_to_predict):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lin_dict = nn.ModuleDict()\n",
    "        for node_type in node_types:\n",
    "            self.lin_dict[node_type] = pyg_nn.Linear(-1, hidden_channels)\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = pyg_nn.HGTConv(hidden_channels, hidden_channels, metadata, num_heads)\n",
    "            self.convs.append(conv)\n",
    "\n",
    "        self.classification_heads = nn.ModuleList([ClassificationHead(n_input_features=hidden_channels, n_classes=1) for _ in range(out_channels)])\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = {\n",
    "            node_type: self.lin_dict[node_type](x).relu_()\n",
    "            for node_type, x in x_dict.items()\n",
    "        }\n",
    "\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "\n",
    "        if isinstance(self.classification_heads, nn.ModuleList):\n",
    "            outputs = torch.cat([head(x_dict[node_to_predict]).squeeze(-1).unsqueeze(1) for head in self.classification_heads], dim=1)\n",
    "            return outputs\n",
    "        else:\n",
    "            return x_dict[node_to_predict]\n",
    "\n",
    "    def get_n_parameters(self):\n",
    "        n_parameters = sum(p.numel() for p in self.parameters())\n",
    "        return n_parameters\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Multi-Task Heterogenous Graph Transformer with {self.get_n_parameters():,} parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0530f55-8b68-4bc2-baac-26b6b98b1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self, raster_model, vector_model, dummy_raster_sample, dummy_vector_sample, n_classes):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        self.raster_model = raster_model\n",
    "        self.vector_model = vector_model\n",
    "        \n",
    "        # both models are already trained and only require gradient for fusion layers\n",
    "        for param in self.raster_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.vector_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # remove classification heads\n",
    "        self.raster_model.classification_heads = nn.Identity()\n",
    "        self.vector_model.classification_heads = nn.Identity()\n",
    "\n",
    "        # pass dummy raster and dummy vector samples through the networks to determine the number of output features\n",
    "        # when the classification heads are missing\n",
    "        out_raster = self.raster_model(dummy_raster_sample.unsqueeze(0))\n",
    "        out_vector = self.vector_model(dummy_vector_sample.x_dict, dummy_vector_sample.edge_index_dict)\n",
    "        n_raster_features = out_raster.shape[1]\n",
    "        n_vector_features = out_vector.shape[1]\n",
    "        \n",
    "        # fusion layer\n",
    "        self.fusion_layer = nn.Linear(n_raster_features + n_vector_features, n_classes)\n",
    "\n",
    "    def forward(self, raster, graph):\n",
    "        raster_output = self.raster_model(raster)\n",
    "        vector_output = self.vector_model(graph.x_dict, graph.edge_index_dict)\n",
    "\n",
    "        # concatenate along feature dimension\n",
    "        combined_features = torch.cat((raster_output, vector_output), dim=1)\n",
    "        result = self.fusion_layer(combined_features)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf4f96a-b9c1-486e-b07d-a83c885ac113",
   "metadata": {},
   "source": [
    "### Elimination model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39ffbd5e-7ad4-4e11-969e-00cf2b35ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,000 samples in the training set.\n",
      "250 samples in the validation set.\n",
      "250 samples in the test set.\n"
     ]
    }
   ],
   "source": [
    "# define path to training, validation and test data for both raster and vector\n",
    "path_to_raster_training_data = \"../data.nosync/raster/training_data/elimination/training\"\n",
    "path_to_vector_training_data = \"../data.nosync/vector/training_data/elimination/training\"\n",
    "path_to_raster_validation_data = \"../data.nosync/raster/training_data/elimination/validation\"\n",
    "path_to_vector_validation_data = \"../data.nosync/vector/training_data/elimination/validation\"\n",
    "path_to_raster_test_data = \"../data.nosync/raster/training_data/elimination/test\"\n",
    "path_to_vector_test_data = \"../data.nosync/vector/training_data/elimination/test\"\n",
    "\n",
    "# define input parameters\n",
    "elimination_operators = [\"elimination\"]\n",
    "n_classes = len(elimination_operators)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# construct training DataLoader\n",
    "training_set = BuildingMultimodalDataset(path_to_raster_training_data, path_to_vector_training_data, operators=elimination_operators)\n",
    "training_loader = DataLoader(dataset=training_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# construct validation DataLoader (no shuffling)\n",
    "validation_set = BuildingMultimodalDataset(path_to_raster_validation_data, path_to_vector_validation_data, operators=elimination_operators)\n",
    "validation_loader = DataLoader(dataset=validation_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# construct test DataLoader (no shuffling)\n",
    "test_set = BuildingMultimodalDataset(path_to_raster_test_data, path_to_vector_test_data, operators=elimination_operators)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"{len(training_set):,} samples in the training set.\")\n",
    "print(f\"{len(validation_set):,} samples in the validation set.\")\n",
    "print(f\"{len(test_set):,} samples in the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50e07385-c1b9-4b98-bec5-39f9090086ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of node features: {'focal_building': 10, 'context_building': 10, 'road': 2}, 1 operators\n"
     ]
    }
   ],
   "source": [
    "# extracting dummy raster sample\n",
    "dummy_raster_sample = training_set[0][0]\n",
    "\n",
    "# extracting the relevant metadata from the data to set up the graph model\n",
    "dummy_vector_sample = training_set[0][1]\n",
    "node_types = dummy_vector_sample.node_types\n",
    "\n",
    "node_features = {}\n",
    "\n",
    "for node_type in node_types:\n",
    "    node_features[node_type] = dummy_vector_sample[node_type][\"x\"].shape[1]\n",
    "\n",
    "n_classes = dummy_vector_sample[\"y\"].shape[1]\n",
    "\n",
    "print(f\"Number of node features: {node_features}, {n_classes} operators\")\n",
    "\n",
    "metadata = dummy_vector_sample.metadata()\n",
    "node_to_predict = \"focal_building\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a60138b4-6c4c-44d8-a6b0-8c0a9c978bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# load the trained raster model\n",
    "raster_model_path = \"../data.nosync/raster/models/elimination\"\n",
    "raster_model_name = \"MultiTaskViT_eli_attachRoadsTrue_26812417p_1000s_10ep_bs16.pth\"\n",
    "raster_model = MultiTaskViT(channels=3, num_classes=n_classes)\n",
    "raster_checkpoint = torch.load(os.path.join(raster_model_path, raster_model_name))\n",
    "raster_model.load_state_dict(raster_checkpoint[\"model_state_dict\"])\n",
    "raster_model.eval()\n",
    "    \n",
    "# load the trained vector model\n",
    "vector_model_path = \"../data.nosync/vector/models/elimination\"\n",
    "vector_model_name = \"MultiTaskHGT_eli_attachRoadsTrue_637219p_10000s_10ep_bs16.pth\"\n",
    "vector_model = MultiTaskHGT(hidden_channels=128, out_channels=n_classes, num_heads=2, num_layers=2, node_types=node_types, \n",
    "                            metadata=metadata, node_to_predict=node_to_predict)\n",
    "\n",
    "# initialize lazy modules\n",
    "with torch.no_grad():\n",
    "    out = vector_model(dummy_vector_sample.x_dict, dummy_vector_sample.edge_index_dict)\n",
    "\n",
    "vector_checkpoint = torch.load(os.path.join(vector_model_path, vector_model_name))\n",
    "vector_model.load_state_dict(vector_checkpoint[\"model_state_dict\"])\n",
    "vector_model.eval()\n",
    "\n",
    "print(\"Models successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50162e44-e4b0-4a2b-88df-7b4d102ee71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the multimodal model\n",
    "multimodal_model = MultimodalModel(raster_model, vector_model, dummy_raster_sample, dummy_vector_sample, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fbdcf0f-3d62-4098-808f-40409d19e3dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3177]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_model(dummy_raster_sample.unsqueeze(0), dummy_vector_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c8169-24e1-4261-ae05-69c411866c45",
   "metadata": {},
   "source": [
    "### Selection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12e667bd-93a1-4456-88a3-78bf9e0110b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,000 samples in the training set.\n",
      "250 samples in the validation set.\n",
      "250 samples in the test set.\n"
     ]
    }
   ],
   "source": [
    "# define path to training, validation and test data for both raster and vector\n",
    "path_to_raster_training_data = \"../data.nosync/raster/training_data/selection/training\"\n",
    "path_to_vector_training_data = \"../data.nosync/vector/training_data/selection/training\"\n",
    "path_to_raster_validation_data = \"../data.nosync/raster/training_data/selection/validation\"\n",
    "path_to_vector_validation_data = \"../data.nosync/vector/training_data/selection/validation\"\n",
    "path_to_raster_test_data = \"../data.nosync/raster/training_data/selection/test\"\n",
    "path_to_vector_test_data = \"../data.nosync/vector/training_data/selection/test\"\n",
    "\n",
    "# define input parameters\n",
    "selection_operators = [\"aggregation\", \"typification\", \"displacement\", \"enlargement\"]\n",
    "n_classes = len(selection_operators)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# construct training DataLoader\n",
    "training_set = BuildingMultimodalDataset(path_to_raster_training_data, path_to_vector_training_data, operators=selection_operators)\n",
    "training_loader = DataLoader(dataset=training_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# construct validation DataLoader (no shuffling)\n",
    "validation_set = BuildingMultimodalDataset(path_to_raster_validation_data, path_to_vector_validation_data, operators=selection_operators)\n",
    "validation_loader = DataLoader(dataset=validation_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# construct test DataLoader (no shuffling)\n",
    "test_set = BuildingMultimodalDataset(path_to_raster_test_data, path_to_vector_test_data, operators=selection_operators)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"{len(training_set):,} samples in the training set.\")\n",
    "print(f\"{len(validation_set):,} samples in the validation set.\")\n",
    "print(f\"{len(test_set):,} samples in the test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87923589-d8e7-4d35-aea4-c4b99a05e3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of node features: {'focal_building': 10, 'context_building': 10, 'road': 2}, 4 operators\n"
     ]
    }
   ],
   "source": [
    "# extracting dummy raster sample\n",
    "dummy_raster_sample = training_set[0][0]\n",
    "\n",
    "# extracting the relevant metadata from the data to set up the graph model\n",
    "dummy_vector_sample = training_set[0][1]\n",
    "node_types = dummy_vector_sample.node_types\n",
    "\n",
    "node_features = {}\n",
    "\n",
    "for node_type in node_types:\n",
    "    node_features[node_type] = dummy_vector_sample[node_type][\"x\"].shape[1]\n",
    "\n",
    "n_classes = dummy_vector_sample[\"y\"].shape[1]\n",
    "\n",
    "print(f\"Number of node features: {node_features}, {n_classes} operators\")\n",
    "\n",
    "metadata = dummy_vector_sample.metadata()\n",
    "node_to_predict = \"focal_building\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d9050ba-ba6e-421c-b9f9-3e30da0f7ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models successfully loaded.\n"
     ]
    }
   ],
   "source": [
    "# load the trained raster model\n",
    "raster_model_path = \"../data.nosync/raster/models/selection\"\n",
    "raster_model_name = \"MultiTaskViT_sel_attachRoadsTrue_26813956p_1000s_10ep_bs16.pth\"\n",
    "raster_model = MultiTaskViT(channels=3, num_classes=4)\n",
    "raster_checkpoint = torch.load(os.path.join(raster_model_path, raster_model_name))\n",
    "raster_model.load_state_dict(raster_checkpoint[\"model_state_dict\"])\n",
    "raster_model.eval()\n",
    "    \n",
    "# load the trained vector model\n",
    "vector_model_path = \"../data.nosync/vector/models/selection\"\n",
    "vector_model_name = \"MultiTaskHGT_sel_attachRoadsTrue_662182p_10000s_10ep_bs16.pth\"\n",
    "vector_model = MultiTaskHGT(hidden_channels=128, out_channels=n_classes, num_heads=2, num_layers=2, node_types=node_types, \n",
    "                            metadata=metadata, node_to_predict=node_to_predict)\n",
    "\n",
    "# initialize lazy modules\n",
    "with torch.no_grad():\n",
    "    out = vector_model(dummy_vector_sample.x_dict, dummy_vector_sample.edge_index_dict)\n",
    "\n",
    "vector_checkpoint = torch.load(os.path.join(vector_model_path, vector_model_name))\n",
    "vector_model.load_state_dict(vector_checkpoint[\"model_state_dict\"])\n",
    "vector_model.eval()\n",
    "\n",
    "print(\"Models successfully loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d9adde2-9c59-40e3-b1d1-cf976b914ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the multimodal model\n",
    "multimodal_model = MultimodalModel(raster_model, vector_model, dummy_raster_sample, dummy_vector_sample, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bef203ce-9b46-476d-9926-fac996bce5eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8012, -0.2070,  0.4219, -0.3853]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multimodal_model(dummy_raster_sample.unsqueeze(0), dummy_vector_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e29e71-00ee-40c3-894a-e5db0c2c9c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
